{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DogCatsRedux Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Rather than importing everything manually, we'll make things easy\n",
    "# and load them all in utils.py, and just import them from there.\n",
    "\n",
    "# 1. install bcolz (pip install bcolz)\n",
    "# 2. install theano (pip install theano)\n",
    "# 3. install keras (pip install keras)\n",
    "# 4. install tensorflow (pip install tensorflow), (pip install tensorflow-gpu)\n",
    "# 5. install protoclbuff (pip install protobuf)\n",
    "%matplotlib inline \n",
    "import utils; reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = '/home/ubuntu/data/redux/' #workspace directory\n",
    "#gen = image.ImageDataGenerator(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.1, horizontal_flip=True)\n",
    "gen=image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(base_path+'train', gen,shuffle=True, batch_size=batch_size)\n",
    "# NB: We don't want to augment or shuffle the validation set\n",
    "val_batches = get_batches(base_path+'valid', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_classes = val_batches.classes\n",
    "trn_classes = batches.classes\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the shape of the labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load the VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Kaggle Submission helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model, test_path, batch_size=64):\n",
    "    test_batches = get_batches(test_path, shuffle=False, batch_size=batch_size, class_mode=None)\n",
    "    return test_batches, model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "def generateKaggleSubmitLink(model, test_path, output_path, batch_size=64):\n",
    "    batches, preds = test(model, test_path, batch_size)\n",
    "    filenames = batches.filenames\n",
    "    isdog = preds[:,1]\n",
    "    ids = [int(f[8:f.find('.')]) for f in filenames]\n",
    "    subm = np.stack([ids, isdog], axis=1)\n",
    "    np.savetxt(output_path, subm, fmt='%d,%.5f', header='id,label', comments='')\n",
    "    return FileLink(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "def generateKaggleSubmitLinkWithClip(model, test_path, output_path, batch_size=64):\n",
    "    batches, preds = test(model, test_path, batch_size)\n",
    "    filenames = batches.filenames\n",
    "    isdog = np.clip(preds[:,1], 0.05, 0.95)\n",
    "    ids = [int(f[8:f.find('.')]) for f in filenames]\n",
    "    subm = np.stack([ids, isdog], axis=1)\n",
    "    np.savetxt(output_path, subm, fmt='%d,%.5f', header='id,label', comments='')\n",
    "    return FileLink(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 3, 224, 224)   0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_1 (ZeroPadding2D)  (None, 3, 226, 226)   0           lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 224, 224)  1792        zeropadding2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_2 (ZeroPadding2D)  (None, 64, 226, 226)  0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 224, 224)  36928       zeropadding2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 112, 112)  0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_3 (ZeroPadding2D)  (None, 64, 114, 114)  0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 112, 112) 73856       zeropadding2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_4 (ZeroPadding2D)  (None, 128, 114, 114) 0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 128, 112, 112) 147584      zeropadding2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 56, 56)   0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_5 (ZeroPadding2D)  (None, 128, 58, 58)   0           maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 256, 56, 56)   295168      zeropadding2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_6 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_7 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 256, 28, 28)   0           convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_8 (ZeroPadding2D)  (None, 256, 30, 30)   0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 512, 28, 28)   1180160     zeropadding2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_9 (ZeroPadding2D)  (None, 512, 30, 30)   0           convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 512, 28, 28)   2359808     zeropadding2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_10 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 512, 28, 28)   2359808     zeropadding2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 512, 14, 14)   0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_11 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_12 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_13 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 512, 7, 7)     0           convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 25088)         0           maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 4096)          102764544   flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 4096)          0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 4096)          16781312    dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 4096)          0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1000)          4097000     dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 138357544\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#opt = RMSprop(lr=0.1, rho=0.7) #what is rho?\n",
    "opt = Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop the last dense layer of 1000 outputs and add a new Dense node of 2 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "23000/23000 [==============================] - 603s - loss: 0.1285 - acc: 0.9660 - val_loss: 0.0370 - val_acc: 0.9890\n",
      "Epoch 2/4\n",
      "23000/23000 [==============================] - 610s - loss: 0.0881 - acc: 0.9773 - val_loss: 0.0377 - val_acc: 0.9880\n",
      "Epoch 3/4\n",
      "23000/23000 [==============================] - 609s - loss: 0.0799 - acc: 0.9792 - val_loss: 0.0475 - val_acc: 0.9865\n",
      "Epoch 4/4\n",
      "23000/23000 [==============================] - 609s - loss: 0.0827 - acc: 0.9785 - val_loss: 0.0386 - val_acc: 0.9865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f94a2baa350>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=4,\n",
    "                    validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vgg model's val accuracy is .9865 after finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(base_path+\"models/ft1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#recreate the architecture\n",
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.load_weights(base_path+\"models/ft1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/data/redux/subm_finetune_last.csv' target='_blank'>/home/ubuntu/data/redux/subm_finetune_last.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/data/redux/subm_finetune_last.csv"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateKaggleSubmitLink(model, base_path+'test', base_path+'subm_finetune_last.csv', batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Received score of 0.18761"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain all the Dense layers\n",
    "While retraining all the dense layers, it is important to load ft1.h5 which is the weights after finetuning the original vgg model.(Replacing last layer with Dense(2)). This is because we don't need to randomly initialize all the weights in the previous layers while training for weights in the last layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "model.load_weights(base_path+\"models/ft1.h5\")\n",
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "for layer in layers: layer.trainable = True\n",
    "for layer in conv_layers: layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#gen = image.ImageDataGenerator(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.1, horizontal_flip=True)\n",
    "gen = image.ImageDataGenerator()\n",
    "batches = get_batches(base_path+'train', gen,shuffle=True, batch_size=batch_size)\n",
    "# NB: We don't want to augment or shuffle the validation set\n",
    "val_batches = get_batches(base_path+'valid', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_8 False\n",
      "zeropadding2d_92 False\n",
      "convolution2d_92 False\n",
      "zeropadding2d_93 False\n",
      "convolution2d_93 False\n",
      "maxpooling2d_37 False\n",
      "zeropadding2d_94 False\n",
      "convolution2d_94 False\n",
      "zeropadding2d_95 False\n",
      "convolution2d_95 False\n",
      "maxpooling2d_38 False\n",
      "zeropadding2d_96 False\n",
      "convolution2d_96 False\n",
      "zeropadding2d_97 False\n",
      "convolution2d_97 False\n",
      "zeropadding2d_98 False\n",
      "convolution2d_98 False\n",
      "maxpooling2d_39 False\n",
      "zeropadding2d_99 False\n",
      "convolution2d_99 False\n",
      "zeropadding2d_100 False\n",
      "convolution2d_100 False\n",
      "zeropadding2d_101 False\n",
      "convolution2d_101 False\n",
      "maxpooling2d_40 False\n",
      "zeropadding2d_102 False\n",
      "convolution2d_102 False\n",
      "zeropadding2d_103 False\n",
      "convolution2d_103 False\n",
      "zeropadding2d_104 False\n",
      "convolution2d_104 False\n",
      "maxpooling2d_42 True\n",
      "flatten_13 True\n",
      "dense_45 True\n",
      "dropout_25 True\n",
      "dense_46 True\n",
      "dropout_26 True\n",
      "dense_47 True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print layer.name,layer.trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 599s - loss: 0.1883 - acc: 0.9751 - val_loss: 0.0926 - val_acc: 0.9865\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 601s - loss: 0.1738 - acc: 0.9780 - val_loss: 0.0910 - val_acc: 0.9865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2d0bbf3590>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=2, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(base_path+\"models/ft2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/redux/models/ft2.h5'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path+\"models/ft2.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#recreate the architecture\n",
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "model.load_weights(base_path+\"models/ft1.h5\")\n",
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "for layer in layers: layer.trainable = True\n",
    "for layer in conv_layers: layer.trainable = False\n",
    "\n",
    "#load weights\n",
    "model.load_weights(base_path+\"models/ft2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/data/redux/subm_finetune_all.csv' target='_blank'>/home/ubuntu/data/redux/subm_finetune_all.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/data/redux/subm_finetune_all.csv"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateKaggleSubmitLink(model, base_path+'test', base_path+'subm_finetune_all.csv', batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Received score of 0.31664 which not an improvement. Hence reduce learning rate, separate conv layer features and train more number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "#loading from ft1.h5 since ft2.h5 was not good\n",
    "model.load_weights(base_path+\"models/ft1.h5\")\n",
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "conv_model = Sequential(conv_layers)\n",
    "# Dense layers - also known as fully connected or 'FC' layers\n",
    "fc_layers = layers[last_conv_idx+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract the conv outpu and save so that if this becomes input other models, training will be MUCH faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(base_path+'train', shuffle=False, batch_size=batch_size)\n",
    "val_batches = get_batches(base_path+'valid', shuffle=False, batch_size=batch_size)\n",
    "\n",
    "val_classes = val_batches.classes\n",
    "trn_classes = batches.classes\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_features = conv_model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "trn_features = conv_model.predict_generator(batches, batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the result (above step is expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(base_path + 'models/train_convlayer_features.bc', trn_features)\n",
    "save_array(base_path + 'models/valid_convlayer_features.bc', val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_features = load_array(base_path + 'models/train_convlayer_features.bc')\n",
    "val_features = load_array(base_path + 'models/valid_convlayer_features.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tranf_wgts(layer): return [o for o in layer.get_weights()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/8\n",
      "23000/23000 [==============================] - 37s - loss: 0.1120 - acc: 0.9663 - val_loss: 0.0350 - val_acc: 0.9855\n",
      "Epoch 2/8\n",
      "23000/23000 [==============================] - 36s - loss: 0.0280 - acc: 0.9907 - val_loss: 0.0376 - val_acc: 0.9875\n",
      "Epoch 3/8\n",
      "23000/23000 [==============================] - 37s - loss: 0.0124 - acc: 0.9960 - val_loss: 0.0362 - val_acc: 0.9880\n",
      "Epoch 4/8\n",
      "23000/23000 [==============================] - 37s - loss: 0.0068 - acc: 0.9977 - val_loss: 0.0415 - val_acc: 0.9885\n",
      "Epoch 5/8\n",
      "23000/23000 [==============================] - 37s - loss: 0.0042 - acc: 0.9984 - val_loss: 0.0351 - val_acc: 0.9890\n",
      "Epoch 6/8\n",
      "23000/23000 [==============================] - 37s - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0426 - val_acc: 0.9875\n",
      "Epoch 7/8\n",
      "23000/23000 [==============================] - 37s - loss: 0.0027 - acc: 0.9989 - val_loss: 0.0584 - val_acc: 0.9870\n",
      "Epoch 8/8\n",
      "23000/23000 [==============================] - 37s - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0524 - val_acc: 0.9875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe8ac36f590>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "for l1,l2 in zip(model.layers, fc_layers): l1.set_weights(tranf_wgts(l2))\n",
    "model.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#load weights\n",
    "model.fit(trn_features, trn_labels, nb_epoch=8, \n",
    "             batch_size=batch_size, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(base_path+'models/ft2_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model might have overfit after epoch 4, but still submit the solution to see the score. We could add more technique on top of an overfit model to reduce it(augmentation, batchnormalization etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first run the conv_model on the test data before we can use the model to do the prediction, \n",
    "since this models expects an inputshape = outputshape of conv model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches = get_batches(base_path+'test', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tst_features = conv_model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_array(base_path + 'models/test_convlayer_features.bc', tst_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst_features = load_array(base_path + 'models/test_convlayer_features.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "def generateKaggleSubmitLink2(model, batches, test_features, output_path, batch_size=64):\n",
    "    preds = model.predict(test_features)\n",
    "    filenames = batches.filenames\n",
    "    isdog = preds[:,1]\n",
    "    ids = [int(f[8:f.find('.')]) for f in filenames]\n",
    "    subm = np.stack([ids, isdog], axis=1)\n",
    "    np.savetxt(output_path, subm, fmt='%d,%.5f', header='id,label', comments='')\n",
    "    return FileLink(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "def generateKaggleSubmitLink2WithClip(model, batches, test_features, output_path, batch_size=64):\n",
    "    preds = model.predict(test_features)\n",
    "    filenames = batches.filenames\n",
    "    isdog = np.clip(preds[:,1], 0.05, 0.95)\n",
    "    ids = [int(f[8:f.find('.')]) for f in filenames]\n",
    "    subm = np.stack([ids, isdog], axis=1)\n",
    "    np.savetxt(output_path, subm, fmt='%d,%.5f', header='id,label', comments='')\n",
    "    return FileLink(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/data/redux/subm_finetune_all2.csv' target='_blank'>/home/ubuntu/data/redux/subm_finetune_all2.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/data/redux/subm_finetune_all2.csv"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateKaggleSubmitLink2(model, test_batches, tst_features, base_path+'subm_finetune_all2.csv', batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Received score of 0.19908 which is better than previous score. But there might be overfitting, but which can be mitigated later. To verify if there is overfitting, retrain with less number of epochs and submit again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "23000/23000 [==============================] - 37s - loss: 0.1117 - acc: 0.9657 - val_loss: 0.0361 - val_acc: 0.9845\n",
      "Epoch 2/3\n",
      "23000/23000 [==============================] - 36s - loss: 0.0268 - acc: 0.9908 - val_loss: 0.0366 - val_acc: 0.9875\n",
      "Epoch 3/3\n",
      "23000/23000 [==============================] - 37s - loss: 0.0118 - acc: 0.9957 - val_loss: 0.0391 - val_acc: 0.9870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe8a4f11890>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "#load again from ft1.h5\n",
    "model.load_weights(base_path+\"models/ft1.h5\")\n",
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "conv_model = Sequential(conv_layers)\n",
    "# Dense layers - also known as fully connected or 'FC' layers\n",
    "fc_layers = layers[last_conv_idx+1:]\n",
    "\n",
    "model = Sequential([\n",
    "    MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "for l1,l2 in zip(model.layers, fc_layers): l1.set_weights(tranf_wgts(l2))\n",
    "model.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#load weights\n",
    "model.fit(trn_features, trn_labels, nb_epoch=3, \n",
    "             batch_size=batch_size, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(base_path+'models/ft3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/data/redux/subm_finetune_all3.csv' target='_blank'>/home/ubuntu/data/redux/subm_finetune_all3.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/data/redux/subm_finetune_all3.csv"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateKaggleSubmitLink2(model, test_batches, tst_features, base_path+'subm_finetune_all3.csv', batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Received score of 0.16935, which is better than previous score of 0.19 so the assumption of overfitting was right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove dropout\n",
    "Dropout is added to remove overfitting, we could try to make the dropout 0 and see if the model performance increases. Will build two models one from ft3.h5(which was trained less number of times => originally less overfitting) and from ft2_2.h5(which was trained with more #epochs => originally more overfit) and compare the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A function to make weights half, this is done since the VGG model had dropout of 50% and since we are going to remove it to avoid underfitting\n",
    "def proc_wgts(layer): return [o/2 for o in layer.get_weights()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ******reduce the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt=Adam(lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the previous weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rebuild the architecture which was used for ft3.h5 and load the weights\n",
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "\n",
    "model.pop()\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "#load again from ft1.h5\n",
    "model.load_weights(base_path+\"models/ft1.h5\")\n",
    "\n",
    "for layer in model.layers: layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "\n",
    "fc_model_tmp = Sequential([\n",
    "    MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "    ])\n",
    "fc_model_tmp.load_weights(base_path+'models/ft3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of Conv layer becomes the features(input for the FC model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(base_path+'train', shuffle=False, batch_size=batch_size)\n",
    "val_batches = get_batches(base_path+'valid', shuffle=False, batch_size=batch_size)\n",
    "\n",
    "val_classes = val_batches.classes\n",
    "trn_classes = batches.classes\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)\n",
    "trn_features = load_array(base_path + 'models/train_convlayer_features.bc')\n",
    "val_features = load_array(base_path + 'models/valid_convlayer_features.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the FC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc_model = Sequential([\n",
    "    MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.),\n",
    "    Dense(2, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l1,l2 in zip(fc_model.layers, fc_model_tmp.layers) : l1.set_weights(proc_wgts(l2))\n",
    "fc_model.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 31s - loss: 0.0187 - acc: 0.9941 - val_loss: 0.0288 - val_acc: 0.9890\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 31s - loss: 0.0014 - acc: 0.9998 - val_loss: 0.0319 - val_acc: 0.9860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9acb7f4c90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.fit(trn_features, trn_labels, nb_epoch=2, \n",
    "             batch_size=batch_size, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the resulting weights\n",
    "fc_model.save_weights(base_path+\"models/no_dropout_ft3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_model.load_weights(base_path+\"models/no_dropout_ft3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the model to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/data/redux/subm_no_dropout_ft3.csv' target='_blank'>/home/ubuntu/data/redux/subm_no_dropout_ft3.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/data/redux/subm_no_dropout_ft3.csv"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateKaggleSubmitLink2(fc_model, test_batches, tst_features, base_path+'subm_no_dropout_ft3.csv', batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Received score of  0.16610. slight improvement over 0.16935"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from ft2_2.h5 (originally overfit model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rebuild the architecture which was used for ft3.h5 and load the weights\n",
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "\n",
    "model.pop()\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "#load again from ft1.h5\n",
    "model.load_weights(base_path+\"models/ft1.h5\")\n",
    "\n",
    "for layer in model.layers: layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "\n",
    "fc_model_tmp = Sequential([\n",
    "    MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "    ])\n",
    "fc_model_tmp.load_weights(base_path+'models/ft2_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc_model = Sequential([\n",
    "    MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.),\n",
    "    Dense(2, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l1,l2 in zip(fc_model.layers, fc_model_tmp.layers) : l1.set_weights(proc_wgts(l2))\n",
    "fc_model.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 31s - loss: 0.0076 - acc: 0.9978 - val_loss: 0.0378 - val_acc: 0.9880\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 30s - loss: 0.0010 - acc: 0.9999 - val_loss: 0.0427 - val_acc: 0.9895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f999f5a8910>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.fit(trn_features, trn_labels, nb_epoch=2, \n",
    "             batch_size=batch_size, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_model.save_weights(base_path+\"models/no_dropout_ft2_2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the model to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_model.load_weights(base_path+\"models/no_dropout_ft2_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/data/redux/subm_no_dropout_ft2_2.csv' target='_blank'>/home/ubuntu/data/redux/subm_no_dropout_ft2_2.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/data/redux/subm_no_dropout_ft2_2.csv"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateKaggleSubmitLink2(fc_model, test_batches, tst_features, base_path+'subm_no_dropout_ft2_2.csv', batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Received score of 0.18027 which is better than 0.19908 but not as good as removing dropout from ft3.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/data/redux/subm_no_dropout_ft2_2_clip.csv' target='_blank'>/home/ubuntu/data/redux/subm_no_dropout_ft2_2_clip.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/data/redux/subm_no_dropout_ft2_2_clip.csv"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateKaggleSubmitLink2WithClip(fc_model, test_batches, tst_features, base_path+'subm_no_dropout_ft2_2_clip.csv', batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "Start from no_dropout_ft3.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator(rotation_range=15, width_shift_range=0.1, \n",
    "                               height_shift_range=0.1, zoom_range=0.1, horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(base_path+'train', gen, batch_size=batch_size)\n",
    "# NB: We don't want to augment or shuffle the validation set\n",
    "val_batches = get_batches(base_path+'valid', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need not train all the conv layers but we need to pass the input through the conv layers(instead of splittig conv and fc layers) since when data augmentation is applied, randomized changes are applied to each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rebuild the architecture which was used for ft1.h5 and load the weights\n",
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "\n",
    "model.pop()\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "#load again from ft1.h5\n",
    "model.load_weights(base_path+\"models/ft1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rebuild the architecture which was used for no_dropout_ft3.h5 and load the weights\n",
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "conv_model=Sequential(conv_layers)\n",
    "\n",
    "fc_model = Sequential([\n",
    "    MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.),\n",
    "    Dense(2, activation='softmax')\n",
    "    ])\n",
    "fc_model.load_weights(base_path+'models/no_dropout_ft3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine conv_model with fc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_model.add(fc_model)\n",
    "for layer in conv_model.layers: layer.trainable = False\n",
    "conv_model.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "23000/23000 [==============================] - 604s - loss: 0.0259 - acc: 0.9905 - val_loss: 0.0319 - val_acc: 0.9860\n",
      "Epoch 2/8\n",
      "23000/23000 [==============================] - 602s - loss: 0.0263 - acc: 0.9903 - val_loss: 0.0319 - val_acc: 0.9860\n",
      "Epoch 3/8\n",
      "23000/23000 [==============================] - 602s - loss: 0.0277 - acc: 0.9904 - val_loss: 0.0319 - val_acc: 0.9860\n",
      "Epoch 4/8\n",
      "23000/23000 [==============================] - 602s - loss: 0.0266 - acc: 0.9898 - val_loss: 0.0319 - val_acc: 0.9860\n",
      "Epoch 5/8\n",
      "23000/23000 [==============================] - 602s - loss: 0.0270 - acc: 0.9897 - val_loss: 0.0319 - val_acc: 0.9860\n",
      "Epoch 6/8\n",
      "23000/23000 [==============================] - 602s - loss: 0.0252 - acc: 0.9909 - val_loss: 0.0319 - val_acc: 0.9860\n",
      "Epoch 7/8\n",
      "23000/23000 [==============================] - 602s - loss: 0.0277 - acc: 0.9902 - val_loss: 0.0319 - val_acc: 0.9860\n",
      "Epoch 8/8\n",
      "23000/23000 [==============================] - 602s - loss: 0.0268 - acc: 0.9902 - val_loss: 0.0319 - val_acc: 0.9860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f999034ef10>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=8, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model.save_weights(base_path+'models/data_aug.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model.load_weights(base_path+'models/data_aug.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/data/redux/subm_data_aug.csv' target='_blank'>/home/ubuntu/data/redux/subm_data_aug.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/data/redux/subm_data_aug.csv"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateKaggleSubmitLink(conv_model, base_path+'test', base_path+'subm_data_aug.csv', batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Received score of  0.16610."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/data/redux/subm_data_aug.csv' target='_blank'>/home/ubuntu/data/redux/subm_data_aug.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/data/redux/subm_data_aug.csv"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateKaggleSubmitLinkWithClip(conv_model, base_path+'test', base_path+'subm_data_aug.csv', batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Received score of 0.08761"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "Batch normalization is a normalization technique used for regulariation, which reduces overfitting. BN makes sure that activations do not become too high. Before BN, only input could be normalized by subtracting the mean and dividing by the Standard deviation. \n",
    "\n",
    "Other forms of normalization could not be applied here, because during learning the normalized/pruned weights will be re-learned by the model.\n",
    "\n",
    "Add two more trainable parameters to each layer - one to multiply all activations to set an arbitrary standard deviation, and one to add to all activations to set an arbitary mean\n",
    "Incorporate both the normalization, and the learnt multiply/add parameters, into the gradient calculations during backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rebuild the architecture which was used for ft1.h5 and load the weights\n",
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "\n",
    "model.pop()\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "#load again from ft1.h5\n",
    "model.load_weights(base_path+\"models/ft1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rebuild the architecture which was used for no_dropout_ft3.h5 and load the weights\n",
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "conv_model=Sequential(conv_layers)\n",
    "\n",
    "fc_model = Sequential([\n",
    "    MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.),\n",
    "    Dense(2, activation='softmax')\n",
    "    ])\n",
    "fc_model.load_weights(base_path+'models/no_dropout_ft3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model.add(fc_model)\n",
    "for layer in conv_model.layers: layer.trainable = False\n",
    "conv_model.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model.load_weights(base_path+'models/data_aug.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 14, 14)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layers[-1].output_shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(p),\n",
    "        BatchNormalization(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(p),\n",
    "        BatchNormalization(),\n",
    "        Dense(2, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.compile(Adam(lr=0.00001), 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/8\n",
      "23000/23000 [==============================] - 66s - loss: 0.1863 - acc: 0.9210 - val_loss: 0.0480 - val_acc: 0.9820\n",
      "Epoch 2/8\n",
      "23000/23000 [==============================] - 66s - loss: 0.0841 - acc: 0.9672 - val_loss: 0.0410 - val_acc: 0.9840\n",
      "Epoch 3/8\n",
      "23000/23000 [==============================] - 66s - loss: 0.0650 - acc: 0.9764 - val_loss: 0.0389 - val_acc: 0.9850\n",
      "Epoch 4/8\n",
      "23000/23000 [==============================] - 66s - loss: 0.0488 - acc: 0.9810 - val_loss: 0.0392 - val_acc: 0.9880\n",
      "Epoch 5/8\n",
      "23000/23000 [==============================] - 66s - loss: 0.0390 - acc: 0.9859 - val_loss: 0.0409 - val_acc: 0.9865\n",
      "Epoch 6/8\n",
      "23000/23000 [==============================] - 66s - loss: 0.0315 - acc: 0.9875 - val_loss: 0.0393 - val_acc: 0.9875\n",
      "Epoch 7/8\n",
      "23000/23000 [==============================] - 66s - loss: 0.0238 - acc: 0.9911 - val_loss: 0.0398 - val_acc: 0.9885\n",
      "Epoch 8/8\n",
      "23000/23000 [==============================] - 66s - loss: 0.0208 - acc: 0.9924 - val_loss: 0.0414 - val_acc: 0.9870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd7b4aefe50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(trn_features, trn_labels, nb_epoch=8, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.save_weights(base_path+'models/bn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/data/redux/subm_bn_clip.csv' target='_blank'>/home/ubuntu/data/redux/subm_bn_clip.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/data/redux/subm_bn_clip.csv"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateKaggleSubmitLink2WithClip(bn_model, test_batches, tst_features, base_path+'subm_bn_clip.csv', batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Received score 0.09358"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = bn_model.predict(val_features)\n",
    "our_label_cont = preds[:,1]\n",
    "our_lable_binary = np.round(1-our_label_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_lable_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   7  972]\n",
      " [1002   19]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAAGbCAYAAACI1+plAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd7adecec90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(val_classes, our_lable_binary)\n",
    "plot_confusion_matrix(cm, {'cat':0, 'dog':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3d3a388-7e2a-4151-9b50-c20498fceacc",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe47bd48-3414-4657-92e7-8b8d6cb0df00",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
