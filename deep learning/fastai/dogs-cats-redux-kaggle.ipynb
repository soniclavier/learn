{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DogCatsRedux Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Rather than importing everything manually, we'll make things easy\n",
    "# and load them all in utils.py, and just import them from there.\n",
    "\n",
    "# 1. install bcolz (pip install bcolz)\n",
    "# 2. install theano (pip install theano)\n",
    "# 3. install keras (pip install keras)\n",
    "# 4. install tensorflow (pip install tensorflow), (pip install tensorflow-gpu)\n",
    "# 5. install protoclbuff (pip install protobuf)\n",
    "%matplotlib inline \n",
    "import utils; reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = '/home/ubuntu/data/redux/' #workspace directory\n",
    "#gen = image.ImageDataGenerator(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.1, horizontal_flip=True)\n",
    "gen=image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(base_path+'train', gen,shuffle=True, batch_size=batch_size)\n",
    "# NB: We don't want to augment or shuffle the validation set\n",
    "val_batches = get_batches(base_path+'valid', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_classes = val_batches.classes\n",
    "trn_classes = batches.classes\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the shape of the labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load the VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Kaggle Submission helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model, test_path, batch_size=64):\n",
    "    test_batches = get_batches(test_path, shuffle=False, batch_size=batch_size, class_mode=None)\n",
    "    return test_batches, model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "def generateKaggleSubmitLink(model, test_path, output_path, batch_size=64):\n",
    "    batches, preds = test(model, test_path, batch_size)\n",
    "    filenames = batches.filenames\n",
    "    isdog = preds[:,1]\n",
    "    ids = [int(f[8:f.find('.')]) for f in filenames]\n",
    "    subm = np.stack([ids, isdog], axis=1)\n",
    "    np.savetxt(output_path, subm, fmt='%d,%.5f', header='id,label', comments='')\n",
    "    return FileLink(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 3, 224, 224)   0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_1 (ZeroPadding2D)  (None, 3, 226, 226)   0           lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 224, 224)  1792        zeropadding2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_2 (ZeroPadding2D)  (None, 64, 226, 226)  0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 224, 224)  36928       zeropadding2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 112, 112)  0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_3 (ZeroPadding2D)  (None, 64, 114, 114)  0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 112, 112) 73856       zeropadding2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_4 (ZeroPadding2D)  (None, 128, 114, 114) 0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 128, 112, 112) 147584      zeropadding2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 56, 56)   0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_5 (ZeroPadding2D)  (None, 128, 58, 58)   0           maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 256, 56, 56)   295168      zeropadding2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_6 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_7 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 256, 28, 28)   0           convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_8 (ZeroPadding2D)  (None, 256, 30, 30)   0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 512, 28, 28)   1180160     zeropadding2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_9 (ZeroPadding2D)  (None, 512, 30, 30)   0           convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 512, 28, 28)   2359808     zeropadding2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_10 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 512, 28, 28)   2359808     zeropadding2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 512, 14, 14)   0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_11 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_12 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_13 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 512, 7, 7)     0           convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 25088)         0           maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 4096)          102764544   flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 4096)          0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 4096)          16781312    dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 4096)          0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1000)          4097000     dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 138357544\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#opt = RMSprop(lr=0.1, rho=0.7) #what is rho?\n",
    "opt = Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop the last dense layer of 1000 outputs and add a new Dense node of 2 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "23000/23000 [==============================] - 603s - loss: 0.1285 - acc: 0.9660 - val_loss: 0.0370 - val_acc: 0.9890\n",
      "Epoch 2/4\n",
      "23000/23000 [==============================] - 610s - loss: 0.0881 - acc: 0.9773 - val_loss: 0.0377 - val_acc: 0.9880\n",
      "Epoch 3/4\n",
      "23000/23000 [==============================] - 609s - loss: 0.0799 - acc: 0.9792 - val_loss: 0.0475 - val_acc: 0.9865\n",
      "Epoch 4/4\n",
      "23000/23000 [==============================] - 609s - loss: 0.0827 - acc: 0.9785 - val_loss: 0.0386 - val_acc: 0.9865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f94a2baa350>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=4,\n",
    "                    validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vgg model's val accuracy is .9865 after finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(base_path+\"models/ft1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#recreate the architecture\n",
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.load_weights(base_path+\"models/ft1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/data/redux/subm_finetune_last.csv' target='_blank'>/home/ubuntu/data/redux/subm_finetune_last.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/data/redux/subm_finetune_last.csv"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateKaggleSubmitLink(model, base_path+'test', base_path+'subm_finetune_last.csv', batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Received score of 0.18761"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain all the Dense layers\n",
    "While retraining all the dense layers, it is important to load ft1.h5 which is the weights after finetuning the original vgg model.(Replacing last layer with Dense(2)). This is because we don't need to randomly initialize all the weights in the previous layers while training for weights in the last layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "model.load_weights(base_path+\"models/ft1.h5\")\n",
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "for layer in layers: layer.trainable = True\n",
    "for layer in conv_layers: layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#gen = image.ImageDataGenerator(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.1, horizontal_flip=True)\n",
    "gen = image.ImageDataGenerator()\n",
    "batches = get_batches(base_path+'train', gen,shuffle=True, batch_size=batch_size)\n",
    "# NB: We don't want to augment or shuffle the validation set\n",
    "val_batches = get_batches(base_path+'valid', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_8 False\n",
      "zeropadding2d_92 False\n",
      "convolution2d_92 False\n",
      "zeropadding2d_93 False\n",
      "convolution2d_93 False\n",
      "maxpooling2d_37 False\n",
      "zeropadding2d_94 False\n",
      "convolution2d_94 False\n",
      "zeropadding2d_95 False\n",
      "convolution2d_95 False\n",
      "maxpooling2d_38 False\n",
      "zeropadding2d_96 False\n",
      "convolution2d_96 False\n",
      "zeropadding2d_97 False\n",
      "convolution2d_97 False\n",
      "zeropadding2d_98 False\n",
      "convolution2d_98 False\n",
      "maxpooling2d_39 False\n",
      "zeropadding2d_99 False\n",
      "convolution2d_99 False\n",
      "zeropadding2d_100 False\n",
      "convolution2d_100 False\n",
      "zeropadding2d_101 False\n",
      "convolution2d_101 False\n",
      "maxpooling2d_40 False\n",
      "zeropadding2d_102 False\n",
      "convolution2d_102 False\n",
      "zeropadding2d_103 False\n",
      "convolution2d_103 False\n",
      "zeropadding2d_104 False\n",
      "convolution2d_104 False\n",
      "maxpooling2d_42 True\n",
      "flatten_13 True\n",
      "dense_45 True\n",
      "dropout_25 True\n",
      "dense_46 True\n",
      "dropout_26 True\n",
      "dense_47 True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print layer.name,layer.trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 599s - loss: 0.1883 - acc: 0.9751 - val_loss: 0.0926 - val_acc: 0.9865\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 601s - loss: 0.1738 - acc: 0.9780 - val_loss: 0.0910 - val_acc: 0.9865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2d0bbf3590>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=2, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(base_path+\"models/ft2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/redux/models/ft2.h5'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path+\"models/ft2.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#recreate the architecture\n",
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "model.load_weights(base_path+\"models/ft1.h5\")\n",
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "for layer in layers: layer.trainable = True\n",
    "for layer in conv_layers: layer.trainable = False\n",
    "\n",
    "#load weights\n",
    "model.load_weights(base_path+\"models/ft2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/data/redux/subm_finetune_all.csv' target='_blank'>/home/ubuntu/data/redux/subm_finetune_all.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/data/redux/subm_finetune_all.csv"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateKaggleSubmitLink(model, base_path+'test', base_path+'subm_finetune_all.csv', batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Received score of 0.31664 which not an improvement. Hence reduce learning rate, separate conv layer features and train more number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "#loading from ft1.h5 since ft2.h5 was not good\n",
    "model.load_weights(base_path+\"models/ft1.h5\")\n",
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "conv_model = Sequential(conv_layers)\n",
    "# Dense layers - also known as fully connected or 'FC' layers\n",
    "fc_layers = layers[last_conv_idx+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract the conv outpu and save so that if this becomes input other models, training will be MUCH faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches = get_batches(base_path+'train', shuffle=False, batch_size=batch_size)\n",
    "val_batches = get_batches(base_path+'valid', shuffle=False, batch_size=batch_size)\n",
    "\n",
    "val_classes = val_batches.classes\n",
    "trn_classes = batches.classes\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)\n",
    "val_features = conv_model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "trn_features = conv_model.predict_generator(batches, batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the result (above step is expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(base_path + 'models/train_convlayer_features.bc', trn_features)\n",
    "save_array(base_path + 'models/valid_convlayer_features.bc', val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_features = load_array(base_path + 'models/train_convlayer_features.bc')\n",
    "val_features = load_array(base_path + 'models/valid_convlayer_features.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tranf_wgts(layer): return [o for o in layer.get_weights()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/8\n",
      "23000/23000 [==============================] - 37s - loss: 0.1120 - acc: 0.9663 - val_loss: 0.0350 - val_acc: 0.9855\n",
      "Epoch 2/8\n",
      "23000/23000 [==============================] - 36s - loss: 0.0280 - acc: 0.9907 - val_loss: 0.0376 - val_acc: 0.9875\n",
      "Epoch 3/8\n",
      "23000/23000 [==============================] - 37s - loss: 0.0124 - acc: 0.9960 - val_loss: 0.0362 - val_acc: 0.9880\n",
      "Epoch 4/8\n",
      "23000/23000 [==============================] - 37s - loss: 0.0068 - acc: 0.9977 - val_loss: 0.0415 - val_acc: 0.9885\n",
      "Epoch 5/8\n",
      "23000/23000 [==============================] - 37s - loss: 0.0042 - acc: 0.9984 - val_loss: 0.0351 - val_acc: 0.9890\n",
      "Epoch 6/8\n",
      "23000/23000 [==============================] - 37s - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0426 - val_acc: 0.9875\n",
      "Epoch 7/8\n",
      "23000/23000 [==============================] - 37s - loss: 0.0027 - acc: 0.9989 - val_loss: 0.0584 - val_acc: 0.9870\n",
      "Epoch 8/8\n",
      "23000/23000 [==============================] - 37s - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0524 - val_acc: 0.9875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe8ac36f590>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "for l1,l2 in zip(model.layers, fc_layers): l1.set_weights(tranf_wgts(l2))\n",
    "model.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#load weights\n",
    "model.fit(trn_features, trn_labels, nb_epoch=8, \n",
    "             batch_size=batch_size, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(base_path+'models/ft2_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model might have overfit after epoch 4, but still submit the solution to see the score. We could add more technique on top of an overfit model to reduce it(augmentation, batchnormalization etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first run the conv_model on the test data before we can use the model to do the prediction, \n",
    "since this models expects an inputshape = outputshape of conv model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst_features = conv_model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_array(base_path + 'models/test_convlayer_features.bc', tst_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst_classes = test_batches.filenames\n",
    "tst_labels = onehot(tst_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "def generateKaggleSubmitLink2(model, batches, test_features, output_path, batch_size=64):\n",
    "    preds = model.predict(test_features)\n",
    "    filenames = batches.filenames\n",
    "    isdog = preds[:,1]\n",
    "    ids = [int(f[8:f.find('.')]) for f in filenames]\n",
    "    subm = np.stack([ids, isdog], axis=1)\n",
    "    np.savetxt(output_path, subm, fmt='%d,%.5f', header='id,label', comments='')\n",
    "    return FileLink(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/data/redux/subm_finetune_all2.csv' target='_blank'>/home/ubuntu/data/redux/subm_finetune_all2.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/data/redux/subm_finetune_all2.csv"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateKaggleSubmitLink2(model, test_batches, tst_features, base_path+'subm_finetune_all2.csv', batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Received score of 0.19908 which is better than previous score. But there might be overfitting, but which can be mitigated later. To verify if there is overfitting, retrain with less number of epochs and submit again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "23000/23000 [==============================] - 37s - loss: 0.1117 - acc: 0.9657 - val_loss: 0.0361 - val_acc: 0.9845\n",
      "Epoch 2/3\n",
      "23000/23000 [==============================] - 36s - loss: 0.0268 - acc: 0.9908 - val_loss: 0.0366 - val_acc: 0.9875\n",
      "Epoch 3/3\n",
      "23000/23000 [==============================] - 37s - loss: 0.0118 - acc: 0.9957 - val_loss: 0.0391 - val_acc: 0.9870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe8a4f11890>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "#load again from ft1.h5\n",
    "model.load_weights(base_path+\"models/ft1.h5\")\n",
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "layers = model.layers\n",
    "last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                     if type(layer) is Convolution2D][-1]\n",
    "\n",
    "conv_layers = layers[:last_conv_idx+1]\n",
    "conv_model = Sequential(conv_layers)\n",
    "# Dense layers - also known as fully connected or 'FC' layers\n",
    "fc_layers = layers[last_conv_idx+1:]\n",
    "\n",
    "model = Sequential([\n",
    "    MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "for l1,l2 in zip(model.layers, fc_layers): l1.set_weights(tranf_wgts(l2))\n",
    "model.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#load weights\n",
    "model.fit(trn_features, trn_labels, nb_epoch=3, \n",
    "             batch_size=batch_size, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(base_path+'models/ft3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/data/redux/subm_finetune_all3.csv' target='_blank'>/home/ubuntu/data/redux/subm_finetune_all3.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/data/redux/subm_finetune_all3.csv"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateKaggleSubmitLink2(model, test_batches, tst_features, base_path+'subm_finetune_all3.csv', batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Received score of 0.16935, which is better than previous score of 0.19 so the assumption of overfitting was right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove dropout\n",
    "Since the training accuracy was less than validation accuracy => we are underfitting the data. Dropout was added to avoid overfitting, hence remove it and rebuild the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A function to make weights half, this is done since the VGG model had dropout of 50% and since we are going to remove it to avoid underfitting\n",
    "def proc_wgts(layer): return [o/2 for o in layer.get_weights()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ******reduce the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt=Adam(lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the previous weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "model.pop()\n",
    "#for layer in model.layers: layer.trainable=False\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.load_weights(base_path+\"models/ft2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of Conv layer becomes the features(input for the FC model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(base_path+'train', shuffle=False, batch_size=batch_size)\n",
    "val_batches = get_batches(base_path+'valid', shuffle=False, batch_size=batch_size)\n",
    "\n",
    "val_classes = val_batches.classes\n",
    "trn_classes = batches.classes\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)\n",
    "val_features = conv_model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "trn_features = conv_model.predict_generator(batches, batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the FC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fc_model():\n",
    "    model = Sequential([\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.),\n",
    "        Dense(2, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    for l1,l2 in zip(model.layers, fc_layers): l1.set_weights(proc_wgts(l2))\n",
    "\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_model = get_fc_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/8\n",
      "23000/23000 [==============================] - 36s - loss: 0.0436 - acc: 0.9847 - val_loss: 0.0339 - val_acc: 0.9865\n",
      "Epoch 2/8\n",
      "23000/23000 [==============================] - 36s - loss: 0.0029 - acc: 0.9992 - val_loss: 0.0387 - val_acc: 0.9890\n",
      "Epoch 3/8\n",
      "23000/23000 [==============================] - 37s - loss: 2.5635e-04 - acc: 1.0000 - val_loss: 0.0372 - val_acc: 0.9905\n",
      "Epoch 4/8\n",
      "23000/23000 [==============================] - 37s - loss: 1.0746e-04 - acc: 1.0000 - val_loss: 0.0392 - val_acc: 0.9900\n",
      "Epoch 5/8\n",
      "23000/23000 [==============================] - 36s - loss: 1.4951e-04 - acc: 0.9999 - val_loss: 0.0402 - val_acc: 0.9905\n",
      "Epoch 6/8\n",
      "23000/23000 [==============================] - 37s - loss: 9.3219e-05 - acc: 1.0000 - val_loss: 0.0459 - val_acc: 0.9885\n",
      "Epoch 7/8\n",
      "23000/23000 [==============================] - 37s - loss: 2.1269e-04 - acc: 0.9999 - val_loss: 0.0448 - val_acc: 0.9895\n",
      "Epoch 8/8\n",
      "23000/23000 [==============================] - 37s - loss: 9.6688e-05 - acc: 0.9999 - val_loss: 0.0463 - val_acc: 0.9905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe8e8160f50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.fit(trn_features, trn_labels, nb_epoch=8, \n",
    "             batch_size=batch_size, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the resulting weights\n",
    "model.save_weights(base_path+\"models/no_dropout.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the accuracy is 99% submit the model to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/data/redux/subm_no_dropout.csv' target='_blank'>/home/ubuntu/data/redux/subm_no_dropout.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/data/redux/subm_no_dropout.csv"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateKaggleSubmitLink(model, base_path+'test', base_path+'subm_no_dropout.csv', batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scored 0.31 which was much worser than previous submission(with only last layer finetuning). May be no dropout overfits the data. Submit the previous model which is finetune all dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(val_classes, preds)\n",
    "plot_confusion_matrix(cm, {'cat':0, 'dog':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3d3a388-7e2a-4151-9b50-c20498fceacc",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe47bd48-3414-4657-92e7-8b8d6cb0df00",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
